{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0117ac88c98440b29ab1f452107cbe1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0d592098920140dab61aac5410568c36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6dcb3ec9c624171966bb889808dbcb3","placeholder":"​","style":"IPY_MODEL_100abf072991474abfcee871d2b83f37","value":"100%"}},"100abf072991474abfcee871d2b83f37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d64942a0eaa409a93df049bb594c062":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"401ae722b95c4ff59b836422dbe71edc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d64942a0eaa409a93df049bb594c062","placeholder":"​","style":"IPY_MODEL_8c8721e504cf434eb9e263fb9983969a","value":" 8000/8000 [00:33&lt;00:00, 256.07it/s]"}},"4efbfb7c7cb54276862e5321209d57fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6786c2b0e2614ad389620246cb2178f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d592098920140dab61aac5410568c36","IPY_MODEL_da681e3cc353420cb142d56df0fce231","IPY_MODEL_401ae722b95c4ff59b836422dbe71edc"],"layout":"IPY_MODEL_4efbfb7c7cb54276862e5321209d57fa"}},"8c8721e504cf434eb9e263fb9983969a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da681e3cc353420cb142d56df0fce231":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_decc12da6f5742ec8b7ec7789ee434ab","max":8000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0117ac88c98440b29ab1f452107cbe1a","value":8000}},"decc12da6f5742ec8b7ec7789ee434ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6dcb3ec9c624171966bb889808dbcb3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":48666,"databundleVersionId":5149531,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task description\n- Classify the speakers of given features.\n- Main goal: Learn how to use transformer.\n- Baselines:\n  - Easy: Run sample code and know how to use transformer.\n  - Medium: Know how to adjust parameters of transformer.\n  - Strong: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer. \n  - Boss: Implement [Self-Attention Pooling](https://arxiv.org/pdf/2008.01077v1.pdf) & [Additive Margin Softmax](https://arxiv.org/pdf/1801.05599.pdf) to further boost the performance.\n\n- Other links\n  - Competiton: [link](https://www.kaggle.com/t/49ea0c385a974db5919ec67299ba2e6b)\n  - Slide: [link](https://docs.google.com/presentation/d/1LDAW0GGrC9B6D7dlNdYzQL6D60-iKgFr/edit?usp=sharing&ouid=104280564485377739218&rtpof=true&sd=true)\n  - Data: [link](https://github.com/googly-mingto/ML2023HW4/releases)\n\n# Download dataset\n- Data is [here](https://drive.google.com/drive/folders/1vI1kuLB-q1VilIftiwnPOCAeOOFfBZge?usp=sharing)","metadata":{"id":"C_jdZ5vHJ4A9"}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nset_seed(87)","metadata":{"id":"E6burzCXIyuA","execution":{"iopub.status.busy":"2024-04-30T13:25:20.907874Z","iopub.execute_input":"2024-04-30T13:25:20.908451Z","iopub.status.idle":"2024-04-30T13:25:24.209363Z","shell.execute_reply.started":"2024-04-30T13:25:20.908419Z","shell.execute_reply":"2024-04-30T13:25:24.208447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data\n\n## Dataset\n- Original dataset is [Voxceleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html).\n- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb2.\n- We randomly select 600 speakers from Voxceleb2.\n- Then preprocess the raw waveforms into mel-spectrograms.\n\n- Args:\n  - data_dir: The path to the data directory.\n  - metadata_path: The path to the metadata.\n  - segment_len: The length of audio segment for training. \n- The architecture of data directory \\\\\n  - data directory \\\\\n  |---- metadata.json \\\\\n  |---- testdata.json \\\\\n  |---- mapping.json \\\\\n  |---- uttr-{random string}.pt \\\\\n\n- The information in metadata\n  - \"n_mels\": The dimention of mel-spectrogram.\n  - \"speakers\": A dictionary. \n    - Key: speaker ids.\n    - value: \"feature_path\" and \"mel_len\"\n\n\nFor efficiency, we segment the mel-spectrograms into segments in the traing step.","metadata":{"id":"k7dVbxW2LASN"}},{"cell_type":"code","source":"import os\nimport json\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass myDataset(Dataset):\n    def __init__(self, data_dir, segment_len=128):\n        self.data_dir = data_dir\n        self.segment_len = segment_len\n\n        # Load the mapping from speaker neme to their corresponding id.\n        mapping_path = Path(data_dir) / \"mapping.json\"\n        mapping = json.load(mapping_path.open())\n        self.speaker2id = mapping[\"speaker2id\"]\n\n        # Load metadata of training data.\n        metadata_path = Path(data_dir) / \"metadata.json\"\n        metadata = json.load(open(metadata_path))[\"speakers\"]\n\n        # Get the total number of speaker.\n        self.speaker_num = len(metadata.keys())\n        self.data = []\n        for speaker in metadata.keys():\n            for utterances in metadata[speaker]:\n                self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        feat_path, speaker = self.data[index]\n        # Load preprocessed mel-spectrogram.\n        mel = torch.load(os.path.join(self.data_dir, feat_path))\n\n        # Segmemt mel-spectrogram into \"segment_len\" frames.\n        if len(mel) > self.segment_len:\n            # Randomly get the starting point of the segment.\n            start = random.randint(0, len(mel) - self.segment_len)\n            # Get a segment with \"segment_len\" frames.\n            mel = torch.FloatTensor(mel[start : start + self.segment_len])\n        else:\n            mel = torch.FloatTensor(mel)\n        # Turn the speaker id into long for computing loss later.\n        speaker = torch.FloatTensor([speaker]).long()\n        return mel, speaker\n\n    def get_speaker_number(self):\n        return self.speaker_num","metadata":{"id":"KpuGxl4CI2pr","execution":{"iopub.status.busy":"2024-04-30T13:25:27.300273Z","iopub.execute_input":"2024-04-30T13:25:27.300828Z","iopub.status.idle":"2024-04-30T13:25:27.317135Z","shell.execute_reply.started":"2024-04-30T13:25:27.300795Z","shell.execute_reply":"2024-04-30T13:25:27.315874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataloader\n- Split dataset into training dataset(90%) and validation dataset(10%).\n- Create dataloader to iterate the data.","metadata":{"id":"668hverTMlGN"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\n\n\ndef collate_batch(batch):\n\t# Process features within a batch.\n\t\"\"\"Collate a batch of data.\"\"\"\n\tmel, speaker = zip(*batch)\n\t# Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n\tmel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n\t# mel: (batch size, length, 40)\n\treturn mel, torch.FloatTensor(speaker).long()\n\n\ndef get_dataloader(data_dir, batch_size, n_workers):\n\t\"\"\"Generate dataloader\"\"\"\n\tdataset = myDataset(data_dir)\n\tspeaker_num = dataset.get_speaker_number()\n\t# Split dataset into training dataset and validation dataset\n\ttrainlen = int(0.9 * len(dataset))\n\tlengths = [trainlen, len(dataset) - trainlen]\n\ttrainset, validset = random_split(dataset, lengths)\n\n\ttrain_loader = DataLoader(\n\t\ttrainset,\n\t\tbatch_size=batch_size,\n\t\tshuffle=True,\n\t\tdrop_last=True,\n\t\tnum_workers=n_workers,\n\t\tpin_memory=True,\n\t\tcollate_fn=collate_batch,\n\t)\n\tvalid_loader = DataLoader(\n\t\tvalidset,\n\t\tbatch_size=batch_size,\n\t\tnum_workers=n_workers,\n\t\tdrop_last=True,\n\t\tpin_memory=True,\n\t\tcollate_fn=collate_batch,\n\t)\n\n\treturn train_loader, valid_loader, speaker_num","metadata":{"id":"B7c2gZYoJDRS","execution":{"iopub.status.busy":"2024-04-30T13:25:35.057531Z","iopub.execute_input":"2024-04-30T13:25:35.057905Z","iopub.status.idle":"2024-04-30T13:25:35.066758Z","shell.execute_reply.started":"2024-04-30T13:25:35.057876Z","shell.execute_reply":"2024-04-30T13:25:35.065907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- TransformerEncoderLayer:\n  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n  - Parameters:\n    - d_model: the number of expected features of the input (required).\n\n    - nhead: the number of heads of the multiheadattention models (required).\n\n    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n\n    - dropout: the dropout value (default=0.1).\n\n    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n\n- TransformerEncoder:\n  - TransformerEncoder is a stack of N transformer encoder layers\n  - Parameters:\n    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n\n    - num_layers: the number of sub-encoder-layers in the encoder (required).\n\n    - norm: the layer normalization component (optional).","metadata":{"id":"5FOSZYxrMqhc"}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio.models.conformer as conformer\n\nclass Classifier(nn.Module):\n\tdef __init__(self, d_model=512, n_spks=600, dropout=0.1):\n\t\tsuper().__init__()\n\t\t# Project the dimension of features from that of input into d_model.\n\t\tself.prenet = nn.Linear(40, d_model)\n\t\t# TODO:\n\t\t#   Change Transformer to Conformer.\n\t\t#   https://arxiv.org/abs/2005.08100\n# \t\tself.encoder_layer = nn.TransformerEncoderLayer(\n# \t\t\td_model=d_model, dim_feedforward=256, nhead=16\n# \t\t)\n\t\t# self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n        \n        # Use Conformer output ((batch size, length, d_model),(length))\n\t\tself.encoder_layer = conformer.Conformer(\n\t\t\t\t\t\tinput_dim=d_model,\n\t\t\t\t\t\tnum_heads=16,\n\t\t\t\t\t\tffn_dim=2048,\n\t\t\t\t\t\tnum_layers=1,\n\t\t\t\t\t\tdepthwise_conv_kernel_size=31,\n\t\t\t\t\t\tdropout=dropout,\n\t\t\t\t\t\tuse_group_norm=False,\n\t\t\t\t\t\tconvolution_first=False,\n\t\t)\n\n\t\t# Project the the dimension of features from d_model into speaker nums.\n\t\tself.pred_layer = nn.Sequential(\n# \t\t\tnn.Linear(d_model, 2*d_model),\n# \t\t\tnn.Sigmoid(),\n# \t\t\tnn.Dropout(dropout),\n# \t\t\tnn.Linear(2*d_model, n_spks),\n\t\t\tnn.Linear(d_model, n_spks)\n\t\t)\n\n\t\t# 使用conformer改写encoder_layer\n\t\t# self.encoder_layer = ConformerEncoderLayer(\n\t\t# \td_model=d_model, nhead=2, dim_feedforward=256\n\t\t# )\n\tdef forward(self, mels):\n\t\t\"\"\"\n\t\targs:\n\t\t\tmels: (batch size, length, 40)\n\t\treturn:\n\t\t\tout: (batch size, n_spks)\n\t\t\"\"\"\n\t\t# out: (batch size, length, d_model)\n\t\tout = self.prenet(mels)\n# \t\t# out: (length, batch size, d_model)\n# \t\tout = out.permute(1, 0, 2)\n# \t\t# The encoder layer expect features in the shape of (length, batch size, d_model).\n# \t\tout = self.encoder_layer(out)\n        # Conformer input (batch size, length, d_model), lengths\n\t\tlength = out.shape[1]\n\t\tout, _ = self.encoder_layer(out, torch.full((out.shape[0],), length).to('cuda'))\n# \t\t# out: (batch size, length, d_model)\n# \t\tout = out.transpose(0, 1)\n\t\t# mean pooling\n\t\tstats = out.mean(dim=1)\n\n\t\t# out: (batch, n_spks)\n\t\tout = self.pred_layer(stats)\n\t\treturn out\n\t\n","metadata":{"id":"iXZ5B0EKJGs8","execution":{"iopub.status.busy":"2024-04-30T13:47:30.729238Z","iopub.execute_input":"2024-04-30T13:47:30.729621Z","iopub.status.idle":"2024-04-30T13:47:30.741977Z","shell.execute_reply.started":"2024-04-30T13:47:30.729576Z","shell.execute_reply":"2024-04-30T13:47:30.741080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning rate schedule\n- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n- The warmup schedule\n  - Set learning rate to 0 in the beginning.\n  - The learning rate increases linearly from 0 to initial learning rate during warmup period.","metadata":{"id":"W7yX8JinM5Ly"}},{"cell_type":"code","source":"import math\n\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\ndef get_cosine_schedule_with_warmup(\n\toptimizer: Optimizer,\n\tnum_warmup_steps: int,\n\tnum_training_steps: int,\n\tnum_cycles: float = 0.5,\n\tlast_epoch: int = -1,\n):\n\t\"\"\"\n\tCreate a schedule with a learning rate that decreases following the values of the cosine function between the\n\tinitial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n\tinitial lr set in the optimizer.\n\n\tArgs:\n\t\toptimizer (:class:`~torch.optim.Optimizer`):\n\t\tThe optimizer for which to schedule the learning rate.\n\t\tnum_warmup_steps (:obj:`int`):\n\t\tThe number of steps for the warmup phase.\n\t\tnum_training_steps (:obj:`int`):\n\t\tThe total number of training steps.\n\t\tnum_cycles (:obj:`float`, `optional`, defaults to 0.5):\n\t\tThe number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n\t\tfollowing a half-cosine).\n\t\tlast_epoch (:obj:`int`, `optional`, defaults to -1):\n\t\tThe index of the last epoch when resuming training.\n\n\tReturn:\n\t\t:obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n\t\"\"\"\n\tdef lr_lambda(current_step):\n\t\t# Warmup\n\t\tif current_step < num_warmup_steps:\n\t\t\treturn float(current_step) / float(max(1, num_warmup_steps))\n\t\t# decadence\n\t\tprogress = float(current_step - num_warmup_steps) / float(\n\t\t\tmax(1, num_training_steps - num_warmup_steps)\n\t\t)\n\t\treturn max(\n\t\t\t0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n\t\t)\n\n\treturn LambdaLR(optimizer, lr_lambda, last_epoch)","metadata":{"id":"ykt0N1nVJJi2","execution":{"iopub.status.busy":"2024-04-30T13:26:08.473530Z","iopub.execute_input":"2024-04-30T13:26:08.473945Z","iopub.status.idle":"2024-04-30T13:26:08.482800Z","shell.execute_reply.started":"2024-04-30T13:26:08.473913Z","shell.execute_reply":"2024-04-30T13:26:08.481868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Function\n- Model forward function.","metadata":{"id":"-LN2XkteM_uH"}},{"cell_type":"code","source":"\n\ndef model_fn(batch, model, criterion, device):\n\t\"\"\"Forward a batch through the model.\"\"\"\n\n\tmels, labels = batch\n\tmels = mels.to(device)\n\tlabels = labels.to(device)\n\n\touts = model(mels)\n\n\tloss = criterion(outs, labels)\n\n\t# Get the speaker id with highest probability.\n\tpreds = outs.argmax(1)\n\t# Compute accuracy.\n\taccuracy = torch.mean((preds == labels).float())\n\n\treturn loss, accuracy","metadata":{"id":"N-rr8529JMz0","execution":{"iopub.status.busy":"2024-04-30T13:26:11.596282Z","iopub.execute_input":"2024-04-30T13:26:11.596648Z","iopub.status.idle":"2024-04-30T13:26:11.604786Z","shell.execute_reply.started":"2024-04-30T13:26:11.596618Z","shell.execute_reply":"2024-04-30T13:26:11.603767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate\n- Calculate accuracy of the validation set.","metadata":{"id":"cwM_xyOtNCI2"}},{"cell_type":"code","source":"from tqdm import tqdm\n\n\ndef valid(dataloader, model, criterion, device):\n    \"\"\"Validate on validation set.\"\"\"\n\n    model.eval()\n    running_loss = 0.0\n    running_accuracy = 0.0\n    pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\", position=0)\n\n    for i, batch in enumerate(dataloader):\n        with torch.no_grad():\n            loss, accuracy = model_fn(batch, model, criterion, device)\n            running_loss += loss.item()\n            running_accuracy += accuracy.item()\n\n        pbar.update(dataloader.batch_size)\n        pbar.set_postfix(\n            loss=f\"{running_loss / (i+1):.2f}\",\n            accuracy=f\"{running_accuracy / (i+1):.2f}\",\n        )\n\n    pbar.close()\n    model.train()\n\n    return running_accuracy / len(dataloader)","metadata":{"id":"YAiv6kpdJRTJ","execution":{"iopub.status.busy":"2024-04-30T13:50:12.555052Z","iopub.execute_input":"2024-04-30T13:50:12.555414Z","iopub.status.idle":"2024-04-30T13:50:12.564887Z","shell.execute_reply.started":"2024-04-30T13:50:12.555382Z","shell.execute_reply":"2024-04-30T13:50:12.563537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main function","metadata":{"id":"g6ne9G-eNEdG"}},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.utils.tensorboard import SummaryWriter\n\nglobal exp_name\nexp_name = \"Conformer_SinglePredLayer\"  \n\nwriter = SummaryWriter(\"./runs/\"+exp_name)\n\ndef parse_args():\n\t\"\"\"arguments\"\"\"\n\tconfig = {\n\t\t\"data_dir\": \"/kaggle/input/ml2023springhw4/Dataset\",\n\t\t\"save_path\": exp_name+\".ckpt\",\n\t\t\"batch_size\": 32,\n\t\t\"n_workers\": 8,\n\t\t\"valid_steps\": 2000,\n\t\t\"warmup_steps\": 1000,\n\t\t\"save_steps\": 10000,\n\t\t\"total_steps\": 70000,\n\t}\n\n\treturn config\n\n\ndef main(\n\tdata_dir,\n\tsave_path,\n\tbatch_size,\n\tn_workers,\n\tvalid_steps,\n\twarmup_steps,\n\ttotal_steps,\n\tsave_steps,\n):\n\t\"\"\"Main function.\"\"\"\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")# Use GPU if available\n\tprint(f\"[Info]: Use {device} now!\")# Print device used\n\n\ttrain_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)# Get data\n\ttrain_iterator = iter(train_loader)# Get data iterator\n\tprint(f\"[Info]: Finish loading data!\",flush = True)# Print finish loading data\n\n\tmodel = Classifier(n_spks=speaker_num).to(device)# Create model\n\tcriterion = nn.CrossEntropyLoss()# Define loss function\n\toptimizer = AdamW(model.parameters(), lr=1e-3)# Define optimizer\n\tscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\tprint(f\"[Info]: Finish creating model!\",flush = True)\n\n\tbest_accuracy = -1.0\n\tbest_state_dict = None\n\n\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\", position=0)# Create progress bar\n\n\tfor step in range(total_steps):\n\t\t# Get data\n\t\ttry:\n\t\t\tbatch = next(train_iterator)\n\t\texcept StopIteration:\n\t\t\ttrain_iterator = iter(train_loader)\n\t\t\tbatch = next(train_iterator)\n\n\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n\t\tbatch_loss = loss.item()\n\t\tbatch_accuracy = accuracy.item()\n\n\t\t# Updata model\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tscheduler.step()\n\t\toptimizer.zero_grad()\n\n\t\t# Log\n\t\tpbar.update()\n\t\tpbar.set_postfix(\n\t\t\tloss=f\"{batch_loss:.2f}\",\n\t\t\taccuracy=f\"{batch_accuracy:.2f}\",\n\t\t\tstep=step + 1,\n\t\t)\n\n\t\t# Do validation\n\t\tif (step + 1) % valid_steps == 0:\n\t\t\tpbar.close()\n\n\t\t\tvalid_accuracy = valid(valid_loader, model, criterion, device)\n\n\t\t\t# keep the best model\n\t\t\tif valid_accuracy > best_accuracy:\n\t\t\t\tbest_accuracy = valid_accuracy\n\t\t\t\tbest_state_dict = model.state_dict()\n\n\t\t\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\", position=0)\n\t\t\twriter.add_scalar(\"Acc/valid\", valid_accuracy, step+1)\n\t\t\t\n\t\twriter.add_scalar(\"Acc/train\", batch_accuracy, step+1)\n\t\t\n\n\t\t# Save the best model so far.\n\t\tif (step + 1) % save_steps == 0 and best_state_dict is not None:\n\t\t\ttorch.save(best_state_dict, save_path)\n\t\t\tpbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n\tpbar.close()\n\n\nif __name__ == \"__main__\":\n\tmain(**parse_args())","metadata":{"id":"Usv9s-CuJSG7","outputId":"f4f6a983-3559-4f36-efae-402bbf790473","execution":{"iopub.status.busy":"2024-04-30T13:50:15.072866Z","iopub.execute_input":"2024-04-30T13:50:15.073676Z","iopub.status.idle":"2024-04-30T13:52:34.266007Z","shell.execute_reply.started":"2024-04-30T13:50:15.073637Z","shell.execute_reply":"2024-04-30T13:52:34.264514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n\n## Dataset of inference","metadata":{"id":"NLatBYAhNNMx"}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\n\nclass InferenceDataset(Dataset):\n\tdef __init__(self, data_dir):\n\t\ttestdata_path = Path(data_dir) / \"testdata.json\"\n\t\tmetadata = json.load(testdata_path.open())\n\t\tself.data_dir = data_dir\n\t\tself.data = metadata[\"utterances\"]\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n\n\tdef __getitem__(self, index):\n\t\tutterance = self.data[index]\n\t\tfeat_path = utterance[\"feature_path\"]\n\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n\n\t\treturn feat_path, mel\n\n\ndef inference_collate_batch(batch):\n\t\"\"\"Collate a batch of data.\"\"\"\n\tfeat_paths, mels = zip(*batch)\n\n\treturn feat_paths, torch.stack(mels)","metadata":{"id":"efS4pCmAJXJH","execution":{"iopub.status.busy":"2024-04-30T09:08:01.404020Z","iopub.execute_input":"2024-04-30T09:08:01.404433Z","iopub.status.idle":"2024-04-30T09:08:01.413754Z","shell.execute_reply.started":"2024-04-30T09:08:01.404398Z","shell.execute_reply":"2024-04-30T09:08:01.412508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main funcrion of Inference","metadata":{"id":"tl0WnYwxNK_S"}},{"cell_type":"code","source":"import csv\nfrom tqdm.notebook import tqdm\n\n\ndef parse_args():\n\t\"\"\"arguments\"\"\"\n\tconfig = {\n\t\t\"data_dir\": \"/kaggle/input/ml2023springhw4/Dataset\",\n\t\t\"model_path\": \"./\"+exp_name+\".ckpt\",\n\t\t\"output_path\": \"./\"+exp_name+\".csv\",\n\t}\n\n\treturn config\n\n\ndef main(\n\tdata_dir,\n\tmodel_path,\n\toutput_path,\n):\n\t\"\"\"Main function.\"\"\"\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tprint(f\"[Info]: Use {device} now!\")\n\n\tmapping_path = Path(data_dir) / \"mapping.json\"\n\tmapping = json.load(mapping_path.open())\n\n\tdataset = InferenceDataset(data_dir)\n\tdataloader = DataLoader(\n\t\tdataset,\n\t\tbatch_size=1,\n\t\tshuffle=False,\n\t\tdrop_last=False,\n\t\tnum_workers=8,\n\t\tcollate_fn=inference_collate_batch,\n\t)\n\tprint(f\"[Info]: Finish loading data!\",flush = True)\n\n\tspeaker_num = len(mapping[\"id2speaker\"])\n\tmodel = Classifier(n_spks=speaker_num).to(device)\n\tmodel.load_state_dict(torch.load(model_path))\n\tmodel.eval()\n\tprint(f\"[Info]: Finish creating model!\",flush = True)\n\n\tresults = [[\"Id\", \"Category\"]]\n\tfor feat_paths, mels in tqdm(dataloader):\n\t\twith torch.no_grad():\n\t\t\tmels = mels.to(device)\n\t\t\touts = model(mels)\n\t\t\tpreds = outs.argmax(1).cpu().numpy()\n\t\t\tfor feat_path, pred in zip(feat_paths, preds):\n\t\t\t\tresults.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n\n\twith open(output_path, 'w', newline='') as csvfile:\n\t\twriter = csv.writer(csvfile)\n\t\twriter.writerows(results)\n\n\nif __name__ == \"__main__\":\n\tmain(**parse_args())","metadata":{"id":"i8SAbuXEJb2A","outputId":"3808f409-19c9-426c-dc15-1b88b0c21645","execution":{"iopub.status.busy":"2024-04-30T09:08:04.463390Z","iopub.execute_input":"2024-04-30T09:08:04.463778Z","iopub.status.idle":"2024-04-30T09:08:23.609085Z","shell.execute_reply.started":"2024-04-30T09:08:04.463738Z","shell.execute_reply":"2024-04-30T09:08:23.608092Z"},"trusted":true},"execution_count":null,"outputs":[]}]}